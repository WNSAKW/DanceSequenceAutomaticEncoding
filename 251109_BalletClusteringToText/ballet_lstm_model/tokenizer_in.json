{"class_name": "Tokenizer", "config": {"num_words": null, "filters": "", "lower": true, "split": " ", "char_level": false, "oov_token": "<OOV>", "document_count": 18, "word_counts": "{\"1\": 129, \"0\": 275, \"4\": 92, \"2\": 231, \"|\": 126, \"3\": 67, \"5\": 70}", "word_docs": "{\"0\": 18, \"|\": 18, \"1\": 18, \"2\": 16, \"3\": 17, \"5\": 16, \"4\": 17}", "index_docs": "{\"2\": 18, \"5\": 18, \"4\": 18, \"3\": 16, \"8\": 17, \"7\": 16, \"6\": 17}", "index_word": "{\"1\": \"<OOV>\", \"2\": \"0\", \"3\": \"2\", \"4\": \"1\", \"5\": \"|\", \"6\": \"4\", \"7\": \"5\", \"8\": \"3\"}", "word_index": "{\"<OOV>\": 1, \"0\": 2, \"2\": 3, \"1\": 4, \"|\": 5, \"4\": 6, \"5\": 7, \"3\": 8}"}}