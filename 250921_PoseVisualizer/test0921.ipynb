{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccdd63b-65cb-4ec1-8374-96d4d0308ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a18311-c7a8-4d05-8afa-d88a3c0f168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961fe32f-884c-49be-9f49-938fdf658014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1️⃣ 載入 CSV\n",
    "# -------------------------------\n",
    "def load_track_csv(path):\n",
    "    \"\"\"\n",
    "    假設 CSV 每一列是 [frame_id, joint_id, x, y, score]\n",
    "    或者是展開格式 (T, 33*3)\n",
    "    \"\"\"\n",
    "    data = np.loadtxt(path, delimiter=\",\", skiprows=1)  # 視 CSV 格式調整\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99f9efc-a0f8-4161-aa61-a5142e0af5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2️⃣ 前處理 Pose\n",
    "# -------------------------------\n",
    "def preprocess_pose(data):\n",
    "    \"\"\"\n",
    "    把 CSV 轉成 (T, 33, 3)\n",
    "    假設 data 是 (T, 33*3)\n",
    "    \"\"\"\n",
    "    # 如果 data 已經是 (T, 33, 3)，就不用 reshape\n",
    "    if data.ndim == 2 and data.shape[1] == 33*3:\n",
    "        frames = data.reshape(-1, 33, 3)\n",
    "    elif data.ndim == 3 and data.shape[1] == 33 and data.shape[2] == 3:\n",
    "        frames = data\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected data shape: {data.shape}\")\n",
    "    return frames.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c457632d-425d-4787-b9f7-095fcac5beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3️⃣ Sliding Window\n",
    "# -------------------------------\n",
    "def sliding_window(frames, window=48, step=24):\n",
    "    \"\"\"\n",
    "    frames: (T, 33, 3)\n",
    "    return: (num_slices, window, 33, 3)\n",
    "    \"\"\"\n",
    "    T = frames.shape[0]\n",
    "    slices = []\n",
    "    for start in range(0, T - window + 1, step):\n",
    "        slices.append(frames[start:start + window])\n",
    "    slices = np.array(slices, dtype=np.float32)\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc87403d-77cd-4f9a-84f5-3905adb8e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              frame                               p0  \\\n",
      "0  frame_000350.jpg  ('0.5185', '0.1512', '-2.2092')   \n",
      "1  frame_000351.jpg  ('0.5241', '0.1518', '-2.2169')   \n",
      "2  frame_000352.jpg  ('0.5127', '0.1494', '-2.2461')   \n",
      "3  frame_000353.jpg  ('0.5147', '0.1479', '-2.2896')   \n",
      "4  frame_000354.jpg  ('0.5164', '0.1488', '-2.4700')   \n",
      "\n",
      "                                p1                               p2  \\\n",
      "0  ('0.5506', '0.1301', '-2.2026')  ('0.5668', '0.1285', '-2.2040')   \n",
      "1  ('0.5575', '0.1299', '-2.2069')  ('0.5730', '0.1280', '-2.2084')   \n",
      "2  ('0.5469', '0.1273', '-2.2364')  ('0.5648', '0.1260', '-2.2378')   \n",
      "3  ('0.5476', '0.1248', '-2.2771')  ('0.5650', '0.1236', '-2.2786')   \n",
      "4  ('0.5478', '0.1258', '-2.4605')  ('0.5646', '0.1243', '-2.4620')   \n",
      "\n",
      "                                p3                               p4  \\\n",
      "0  ('0.5809', '0.1271', '-2.2037')  ('0.5015', '0.1272', '-2.2168')   \n",
      "1  ('0.5867', '0.1265', '-2.2081')  ('0.5087', '0.1279', '-2.2250')   \n",
      "2  ('0.5792', '0.1252', '-2.2375')  ('0.4966', '0.1250', '-2.2462')   \n",
      "3  ('0.5797', '0.1227', '-2.2783')  ('0.4985', '0.1222', '-2.2884')   \n",
      "4  ('0.5792', '0.1232', '-2.4618')  ('0.5015', '0.1232', '-2.4700')   \n",
      "\n",
      "                                p5                               p6  \\\n",
      "0  ('0.4854', '0.1238', '-2.2182')  ('0.4712', '0.1205', '-2.2193')   \n",
      "1  ('0.4916', '0.1245', '-2.2265')  ('0.4762', '0.1209', '-2.2275')   \n",
      "2  ('0.4826', '0.1221', '-2.2477')  ('0.4694', '0.1188', '-2.2487')   \n",
      "3  ('0.4847', '0.1192', '-2.2899')  ('0.4728', '0.1159', '-2.2910')   \n",
      "4  ('0.4879', '0.1199', '-2.4716')  ('0.4759', '0.1164', '-2.4727')   \n",
      "\n",
      "                                p7                               p8  ...  \\\n",
      "0  ('0.5934', '0.1129', '-1.7693')  ('0.4489', '0.1055', '-1.8304')  ...   \n",
      "1  ('0.5963', '0.1126', '-1.7619')  ('0.4519', '0.1046', '-1.8401')  ...   \n",
      "2  ('0.5896', '0.1142', '-1.7852')  ('0.4510', '0.1031', '-1.8271')  ...   \n",
      "3  ('0.5904', '0.1129', '-1.8129')  ('0.4561', '0.1020', '-1.8628')  ...   \n",
      "4  ('0.5911', '0.1124', '-1.9983')  ('0.4591', '0.1014', '-2.0395')  ...   \n",
      "\n",
      "                              p23                              p24  \\\n",
      "0  ('0.6248', '0.4448', '0.0700')  ('0.3829', '0.4438', '-0.0704')   \n",
      "1  ('0.6245', '0.4469', '0.0755')  ('0.3814', '0.4484', '-0.0760')   \n",
      "2  ('0.6245', '0.4440', '0.0679')  ('0.3835', '0.4450', '-0.0683')   \n",
      "3  ('0.6215', '0.4485', '0.0901')  ('0.3810', '0.4496', '-0.0906')   \n",
      "4  ('0.6220', '0.4470', '0.0809')  ('0.3824', '0.4467', '-0.0814')   \n",
      "\n",
      "                               p25                              p26  \\\n",
      "0  ('0.7363', '0.6645', '-0.0279')  ('0.2691', '0.6729', '-0.1094')   \n",
      "1  ('0.7364', '0.6662', '-0.0157')  ('0.2703', '0.6748', '-0.0643')   \n",
      "2   ('0.7317', '0.6669', '0.0173')  ('0.2703', '0.6753', '-0.0586')   \n",
      "3   ('0.7285', '0.6678', '0.0427')  ('0.2696', '0.6774', '-0.0868')   \n",
      "4  ('0.7193', '0.6619', '-0.0048')  ('0.2753', '0.6743', '-0.0707')   \n",
      "\n",
      "                              p27                             p28  \\\n",
      "0  ('0.8748', '0.8854', '0.2417')  ('0.1178', '0.8817', '0.1862')   \n",
      "1  ('0.8751', '0.8877', '0.2962')  ('0.1182', '0.8837', '0.2795')   \n",
      "2  ('0.8727', '0.8814', '0.3901')  ('0.1161', '0.8804', '0.3070')   \n",
      "3  ('0.8645', '0.8829', '0.4665')  ('0.1192', '0.8835', '0.2959')   \n",
      "4  ('0.8656', '0.8815', '0.3984')  ('0.1186', '0.8838', '0.3333')   \n",
      "\n",
      "                              p29                             p30  \\\n",
      "0  ('0.8613', '0.9207', '0.2282')  ('0.1066', '0.9102', '0.1866')   \n",
      "1  ('0.8617', '0.9216', '0.2874')  ('0.1049', '0.9117', '0.2844')   \n",
      "2  ('0.8620', '0.9143', '0.3867')  ('0.1026', '0.9077', '0.3137')   \n",
      "3  ('0.8464', '0.9186', '0.4679')  ('0.1069', '0.9132', '0.3037')   \n",
      "4  ('0.8499', '0.9184', '0.3967')  ('0.1043', '0.9108', '0.3419')   \n",
      "\n",
      "                               p31                              p32  \n",
      "0  ('0.8930', '0.9615', '-0.4756')  ('0.1412', '0.9660', '-0.4950')  \n",
      "1  ('0.8924', '0.9632', '-0.4189')  ('0.1426', '0.9690', '-0.4039')  \n",
      "2  ('0.8837', '0.9604', '-0.3286')  ('0.1451', '0.9650', '-0.3901')  \n",
      "3  ('0.8946', '0.9615', '-0.2409')  ('0.1422', '0.9669', '-0.4063')  \n",
      "4  ('0.8960', '0.9636', '-0.3244')  ('0.1401', '0.9671', '-0.3733')  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "(2657, 34)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"track_2.csv\")\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a74357f-baa3-44b7-868f-9d39184244f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_49888\\3382135521.py:11: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  parsed = df.applymap(parse_tuple).to_numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames.shape = (2657, 33, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_track_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    # 去掉 frame 名稱欄\n",
    "    df = df.drop(columns=[\"frame\"])\n",
    "    \n",
    "    # 每個欄位都是一個 tuple 字串，要轉換\n",
    "    def parse_tuple(s):\n",
    "        return tuple(map(float, ast.literal_eval(s)))\n",
    "    \n",
    "    # 把 dataframe 每個元素都轉成 (x, y, z)\n",
    "    parsed = df.applymap(parse_tuple).to_numpy()\n",
    "    \n",
    "    # 現在 parsed.shape = (T, 33)，裡面每個元素是一個 tuple(3,)\n",
    "    # 需要轉成 numpy array (T, 33, 3)\n",
    "    frames = np.array([[list(joint) for joint in row] for row in parsed], dtype=np.float32)\n",
    "    return frames  # (T, 33, 3)\n",
    "\n",
    "# 測試\n",
    "frames = load_track_csv(\"track_2.csv\")\n",
    "print(\"frames.shape =\", frames.shape)  # 應該是 (2657, 33, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "071aadf9-874b-412b-9d32-68820d2a85db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 48, 33, 3)\n"
     ]
    }
   ],
   "source": [
    "slices = sliding_window(frames)\n",
    "print(slices.shape)  # (B, W, 33, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a72df1a-0da3-47c2-9b09-52f8515b118f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string 'frame_000350.jpg' to float64 at row 0, column 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'frame_000350.jpg'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 4️⃣ 主程式\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m frames \u001b[38;5;241m=\u001b[39m preprocess_pose(\u001b[43mload_track_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrack_2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes.shape =\u001b[39m\u001b[38;5;124m\"\u001b[39m, frames\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Debug: 應該是 (T, 33, 3)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m slices \u001b[38;5;241m=\u001b[39m sliding_window(frames)\n",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m, in \u001b[0;36mload_track_csv\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_track_csv\u001b[39m(path):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    假設 CSV 每一列是 [frame_id, joint_id, x, y, score]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    或者是展開格式 (T, 33*3)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 視 CSV 格式調整\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\code-lab\\lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\code-lab\\lib\\site-packages\\numpy\\lib\\npyio.py:1016\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1016\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string 'frame_000350.jpg' to float64 at row 0, column 1."
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 4️⃣ 主程式\n",
    "# -------------------------------\n",
    "frames = preprocess_pose(load_track_csv(\"track_2.csv\"))\n",
    "print(\"frames.shape =\", frames.shape)  # Debug: 應該是 (T, 33, 3)\n",
    "\n",
    "slices = sliding_window(frames)\n",
    "print(\"slices.shape =\", slices.shape)  # Debug: 應該是 (B, W, 33, 3)\n",
    "\n",
    "B, W, J, C = slices.shape\n",
    "slices_flat = slices.reshape(B, W, J * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe75518-38b4-4605-ac65-e70625911700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5️⃣ DataLoader\n",
    "# -------------------------------\n",
    "dataset = TensorDataset(torch.tensor(slices_flat, dtype=torch.float32))\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"DataLoader 準備完成，批次數 =\", len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d59a3bb-134b-462b-82e2-3fce544df937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_49888\\3399312909.py:22: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  parsed = df.applymap(parse_tuple).to_numpy()  # (T, J) of tuples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames.shape = (2657, 33, 3)\n",
      "preprocessed: (2657, 33, 3)\n",
      "slices.shape = (109, 48, 33, 3)\n",
      "flattened: (109, 48, 99)\n",
      "[Epoch 001] loss=1.557305\n",
      "[Epoch 005] loss=0.896720\n",
      "[Epoch 010] loss=0.883558\n",
      "[Epoch 015] loss=0.860917\n",
      "[Epoch 020] loss=0.850787\n",
      "[Epoch 025] loss=0.835136\n",
      "[Epoch 030] loss=0.830535\n",
      "embeddings.shape = (109, 32)\n",
      "cluster labels unique: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "grammar rules: dict_keys(['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9', 'R10', 'R11', 'R12', 'R13', 'R14', 'R15', 'R16', 'R0'])\n",
      "L-system JSON preview: {\n",
      "  \"axiom\": [\n",
      "    \"J\",\n",
      "    \"A\",\n",
      "    \"R2\",\n",
      "    \"R4\",\n",
      "    \"R6\",\n",
      "    \"L\",\n",
      "    \"R4\",\n",
      "    \"R7\",\n",
      "    \"G\",\n",
      "    \"R8\",\n",
      "    \"R8\",\n",
      "    \"F\",\n",
      "    \"F\",\n",
      "    \"R10\",\n",
      "    \"A\",\n",
      "    \"R8\",\n",
      "    \"A\",\n",
      "    \"P\",\n",
      "    \"A\",\n",
      "    \"K\",\n",
      "    \"R11\",\n",
      "    \"G\",\n",
      "    \"C\",\n",
      "    \"I\",\n",
      "    \"R1\",\n",
      "    \"R1\",\n",
      "    \"G\",\n",
      "    \"E\",\n",
      "    \"C\",\n",
      "    \"R7\",\n",
      "    \"E\",\n",
      "    \"I\",\n",
      "    \"R2\",\n",
      "    \"R12\",\n",
      "    \"R13\",\n",
      "    \"M\",\n",
      "    \"J\",\n",
      "    \"M\",\n",
      "    \"R14\",\n",
      "    \"R14\",\n",
      " \n",
      "Saved l_system.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\code-lab\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pipeline_full.py\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# -------------------------------\n",
    "# 1) CSV -> (T, J, C)\n",
    "# -------------------------------\n",
    "def load_track_csv(path: str) -> np.ndarray:\n",
    "    df = pd.read_csv(path)\n",
    "    if \"frame\" in df.columns:\n",
    "        df = df.drop(columns=[\"frame\"])\n",
    "    # parse tuple strings like \"('0.5185','0.1512','-2.2092')\"\n",
    "    def parse_tuple(s):\n",
    "        return tuple(map(float, ast.literal_eval(s)))\n",
    "    parsed = df.applymap(parse_tuple).to_numpy()  # (T, J) of tuples\n",
    "    frames = np.array([[list(j) for j in row] for row in parsed], dtype=np.float32)  # (T, J, 3)\n",
    "    return frames\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Preprocess: hip-centering + shoulder normalization\n",
    "# -------------------------------\n",
    "def preprocess_pose(frames: np.ndarray,\n",
    "                    hip_idx: int = 0,      # choose index for hip / mid-hip\n",
    "                    left_sh_idx: int = 11, # example indices (depends on your keypoints)\n",
    "                    right_sh_idx: int = 12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    frames: (T, J, 3)\n",
    "    return: (T, J, 3) centered & normalized\n",
    "    - translate so hip at origin\n",
    "    - scale so shoulder distance = 1 (or other)\n",
    "    \"\"\"\n",
    "    frames = frames.copy()\n",
    "    # translate: subtract hip position per frame\n",
    "    hip_pos = frames[:, hip_idx:hip_idx+1, :]  # (T,1,3)\n",
    "    frames = frames - hip_pos  # hip at origin\n",
    "\n",
    "    # shoulder distance per frame\n",
    "    shoulder_vec = frames[:, left_sh_idx, :] - frames[:, right_sh_idx, :]  # (T,3)\n",
    "    shoulder_dist = np.linalg.norm(shoulder_vec, axis=1)  # (T,)\n",
    "    # avoid division by zero\n",
    "    shoulder_dist[shoulder_dist == 0] = 1.0\n",
    "    # scale each frame to make shoulder_dist == 1.0 (or any scale)\n",
    "    frames = frames / shoulder_dist[:, None, None]\n",
    "    return frames.astype(np.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Sliding window\n",
    "# -------------------------------\n",
    "def sliding_window(frames: np.ndarray, window: int = 48, step: int = 24) -> np.ndarray:\n",
    "    T = frames.shape[0]\n",
    "    slices = []\n",
    "    for start in range(0, T - window + 1, step):\n",
    "        slices.append(frames[start:start + window])  # (window, J, C)\n",
    "    return np.array(slices, dtype=np.float32)  # (B, W, J, C)\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Flatten for LSTM input\n",
    "# -------------------------------\n",
    "def flatten_slices(slices: np.ndarray) -> np.ndarray:\n",
    "    B, W, J, C = slices.shape\n",
    "    return slices.reshape(B, W, J * C)  # (B, W, feat_dim)\n",
    "\n",
    "# -------------------------------\n",
    "# 5) LSTM Autoencoder (simple)\n",
    "# -------------------------------\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, feat_dim: int, hidden_dim: int = 128, latent_dim: int = 32, num_layers: int = 1):\n",
    "        super().__init__()\n",
    "        self.encoder_lstm = nn.LSTM(input_size=feat_dim, hidden_size=hidden_dim,\n",
    "                                    num_layers=num_layers, batch_first=True)\n",
    "        self.enc_fc = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.dec_fc = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(input_size=hidden_dim, hidden_size=feat_dim,\n",
    "                                    num_layers=num_layers, batch_first=True)\n",
    "        # We'll run a simple decoder: map latent to hidden and repeat across time\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, W, feat_dim)\n",
    "        enc_out, (h_n, c_n) = self.encoder_lstm(x)  # enc_out: (B, W, hidden_dim)\n",
    "        # use last time step hidden\n",
    "        last_hidden = enc_out[:, -1, :]  # (B, hidden_dim)\n",
    "        z = self.enc_fc(last_hidden)     # (B, latent_dim)\n",
    "\n",
    "        # decode: expand z to sequence\n",
    "        dec_hidden = torch.relu(self.dec_fc(z))  # (B, hidden_dim)\n",
    "        # replicate to W steps as \"inputs\" for decoder LSTM\n",
    "        B, W, _ = x.shape\n",
    "        dec_in = dec_hidden.unsqueeze(1).repeat(1, W, 1)  # (B, W, hidden_dim)\n",
    "        dec_out, _ = self.decoder_lstm(dec_in)  # (B, W, feat_dim)\n",
    "        # For simplicity, decoder LSTM's output is final reconstruction\n",
    "        return dec_out, z\n",
    "\n",
    "# -------------------------------\n",
    "# 6) Train function\n",
    "# -------------------------------\n",
    "def train_autoencoder(model: nn.Module, dataloader: DataLoader, epochs: int = 30, lr: float = 1e-3, device: str = \"cpu\"):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            x = batch[0].to(device)  # (B, W, feat)\n",
    "            recon, z = model(x)\n",
    "            loss = criterion(recon, x)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "        avg = total_loss / len(dataloader.dataset)\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch:03d}] loss={avg:.6f}\")\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# 7) Extract embeddings\n",
    "# -------------------------------\n",
    "def extract_embeddings(model: nn.Module, dataloader: DataLoader, device: str = \"cpu\") -> np.ndarray:\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x = batch[0].to(device)\n",
    "            _, z = model(x)\n",
    "            embeddings.append(z.cpu().numpy())\n",
    "    embeddings = np.vstack(embeddings)  # (B, latent_dim)\n",
    "    return embeddings\n",
    "\n",
    "# -------------------------------\n",
    "# 8) Unsupervised clustering -> tokens\n",
    "# -------------------------------\n",
    "def cluster_embeddings(embeddings: np.ndarray, n_clusters: int = 16) -> np.ndarray:\n",
    "    k = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = k.fit_predict(embeddings)\n",
    "    return labels  # (B,)\n",
    "\n",
    "# -------------------------------\n",
    "# 9) Simple Sequitur grammar induction (very minimal)\n",
    "#    Not a full-featured, but finds repeated digrams and replaces them.\n",
    "# -------------------------------\n",
    "def sequitur_from_tokens(tokens: List[int]) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Very small Sequitur-like implementation:\n",
    "    - tokens: sequence of integers (token ids)\n",
    "    - returns a grammar as dict: {'R1': [symbols...], 'R0': [axiom symbols...]}\n",
    "    Note: symbols are ints or rule names 'R#'\n",
    "    \"\"\"\n",
    "    # Start with axiom as token list\n",
    "    grammar = {}\n",
    "    axiom = list(tokens)\n",
    "    rule_id = 1\n",
    "\n",
    "    def find_repeated_digram(seq):\n",
    "        counts = {}\n",
    "        for i in range(len(seq) - 1):\n",
    "            dg = (seq[i], seq[i+1])\n",
    "            counts[dg] = counts.get(dg, 0) + 1\n",
    "        # pick a digram with count >= 2\n",
    "        for dg, c in counts.items():\n",
    "            if c >= 2:\n",
    "                return dg\n",
    "        return None\n",
    "\n",
    "    # iteratively replace repeated digrams\n",
    "    while True:\n",
    "        dg = find_repeated_digram(axiom)\n",
    "        if dg is None:\n",
    "            break\n",
    "        # create rule for dg\n",
    "        rule_name = f\"R{rule_id}\"\n",
    "        grammar[rule_name] = [dg[0], dg[1]]\n",
    "        # replace all occurrences\n",
    "        i = 0\n",
    "        new_axiom = []\n",
    "        while i < len(axiom):\n",
    "            if i < len(axiom) - 1 and (axiom[i], axiom[i+1]) == dg:\n",
    "                new_axiom.append(rule_name)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_axiom.append(axiom[i])\n",
    "                i += 1\n",
    "        axiom = new_axiom\n",
    "        rule_id += 1\n",
    "        # safety break\n",
    "        if rule_id > 200:\n",
    "            break\n",
    "\n",
    "    grammar[\"R0\"] = axiom  # R0 is axiom\n",
    "    return grammar\n",
    "\n",
    "# -------------------------------\n",
    "# 10) Convert grammar -> L-system JSON\n",
    "# -------------------------------\n",
    "def grammar_to_lsystem_json(grammar: Dict[str, List], token_to_symbol: Dict[int, str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Convert grammar (with tokens and rule names) into a JSON-friendly L-system:\n",
    "    - rules: mapping from symbol to expansion (list)\n",
    "    - axiom: initial sequence\n",
    "    token_to_symbol: map token int -> textual symbol (like 'A','B',...)\n",
    "    \"\"\"\n",
    "    # create mapping for token ints\n",
    "    if token_to_symbol is None:\n",
    "        token_to_symbol = {}\n",
    "    def sym(x):\n",
    "        if isinstance(x, str) and x.startswith(\"R\"):\n",
    "            return x  # rule reference stays as is\n",
    "        elif isinstance(x, int):\n",
    "            return token_to_symbol.get(x, f\"T{x}\")\n",
    "        else:\n",
    "            return str(x)\n",
    "\n",
    "    rules_json = {}\n",
    "    for rule, expansion in grammar.items():\n",
    "        rules_json[rule] = [sym(s) for s in expansion]\n",
    "    axiom = [sym(s) for s in grammar.get(\"R0\", [])]\n",
    "    return {\"axiom\": axiom, \"rules\": rules_json}\n",
    "\n",
    "# -------------------------------\n",
    "# 11) Example main pipeline\n",
    "# -------------------------------\n",
    "def main_pipeline(csv_path: str,\n",
    "                  window: int = 48,\n",
    "                  step: int = 24,\n",
    "                  lstm_hidden: int = 128,\n",
    "                  latent_dim: int = 32,\n",
    "                  cluster_k: int = 16,\n",
    "                  device: str = \"cpu\"):\n",
    "    # 1. load\n",
    "    frames = load_track_csv(csv_path)\n",
    "    print(\"frames.shape =\", frames.shape)\n",
    "\n",
    "    # 2. preprocess\n",
    "    frames_p = preprocess_pose(frames, hip_idx=0, left_sh_idx=11, right_sh_idx=12)\n",
    "    print(\"preprocessed:\", frames_p.shape)\n",
    "\n",
    "    # 3. sliding window\n",
    "    slices = sliding_window(frames_p, window=window, step=step)\n",
    "    print(\"slices.shape =\", slices.shape)  # (B, W, J, C)\n",
    "\n",
    "    # 4. flatten\n",
    "    slices_flat = flatten_slices(slices)  # (B, W, feat)\n",
    "    B, W, feat = slices_flat.shape\n",
    "    print(\"flattened:\", slices_flat.shape)\n",
    "\n",
    "    # 5. dataloader\n",
    "    dataset = TensorDataset(torch.tensor(slices_flat, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # 6. model\n",
    "    model = LSTMAutoencoder(feat_dim=feat, hidden_dim=lstm_hidden, latent_dim=latent_dim)\n",
    "    # 7. train\n",
    "    model = train_autoencoder(model, loader, epochs=30, lr=1e-3, device=device)\n",
    "\n",
    "    # 8. extract embeddings\n",
    "    infer_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    embeddings = extract_embeddings(model, infer_loader, device=device)\n",
    "    print(\"embeddings.shape =\", embeddings.shape)\n",
    "\n",
    "    # 9. clustering -> tokens\n",
    "    labels = cluster_embeddings(embeddings, n_clusters=cluster_k)\n",
    "    print(\"cluster labels unique:\", np.unique(labels))\n",
    "\n",
    "    # 10. tokens -> grammar (Sequitur)\n",
    "    tokens = labels.tolist()\n",
    "    grammar = sequitur_from_tokens(tokens)\n",
    "    print(\"grammar rules:\", grammar.keys())\n",
    "\n",
    "    # 11. create token->symbol map (A,B,C...)\n",
    "    symbols = [chr(ord('A') + i) for i in range(26)]\n",
    "    token_to_symbol = {i: symbols[i % len(symbols)] + (str(i//26) if i>=26 else \"\") for i in range(cluster_k)}\n",
    "\n",
    "    lsys = grammar_to_lsystem_json(grammar, token_to_symbol=token_to_symbol)\n",
    "    print(\"L-system JSON preview:\", json.dumps(lsys, indent=2)[:400])\n",
    "\n",
    "    # save JSON\n",
    "    with open(\"l_system.json\", \"w\") as f:\n",
    "        json.dump(lsys, f, indent=2)\n",
    "    print(\"Saved l_system.json\")\n",
    "    return lsys\n",
    "\n",
    "# -------------------------------\n",
    "# If running as script:\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 把路徑換成你的檔案\n",
    "    lsys = main_pipeline(\"track_2.csv\", window=48, step=24,\n",
    "                         lstm_hidden=128, latent_dim=32, cluster_k=16, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff7160-74f3-4b21-b369-3a862a52a9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
